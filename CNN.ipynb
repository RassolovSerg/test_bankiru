{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import pandas as pd\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_data = pd.read_json(\"full_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stop.txt\") as f:\n",
    "    stop = set([w.strip() for w in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25951\n",
       "2     5649\n",
       "3     2129\n",
       "5     1901\n",
       "4      489\n",
       "Name: mark, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_data.mark.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized_text(s):\n",
    "    return \" \".join([w.lower() for w in mystem.lemmatize(s) if (re.match(r\"[а-яА-ЯЁёЙй]{2,}\", w))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_data[\"concat\"] = df_full_data[\"Title\"] + \" \" + df_full_data[\"text\"]\n",
    "#df_full_data[\"hour\"] = df_full_data[\"time\"].apply(lambda x: x.hour)\n",
    "df_full_data[\"prep\"] = df_full_data[\"concat\"].apply(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_data[\"len1\"] = df_full_data[\"concat\"].apply(len)\n",
    "df_full_data[\"len2\"] = df_full_data[\"prep\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214.50164733242892, 4286)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full_data[\"len2\"].mean(), df_full_data[\"len2\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format(\"ft_native_300_ru_wiki_lenta_lemmatize.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(df_full_data.mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_full_data.prep.iloc[:500].values\n",
    "X_train = df_full_data.prep.iloc[500:].values\n",
    "Y_test = Y.iloc[:500].values\n",
    "Y_train = Y.iloc[500:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_seq_train = tokenizer.texts_to_sequences(X_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 400\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, 300))\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v:\n",
    "        embedding_vector = w2v[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 400, 300)          14556600  \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 400, 64)           134464    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 200, 64)           28736     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 14,722,045\n",
      "Trainable params: 165,445\n",
      "Non-trainable params: 14,556,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim,\n",
    "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(5, activation='softmax'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=2, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32057 samples, validate on 3562 samples\n",
      "Epoch 1/8\n",
      "32057/32057 [==============================] - 64s 2ms/step - loss: 0.9310 - acc: 0.7190 - val_loss: 1.1716 - val_acc: 0.5870\n",
      "Epoch 2/8\n",
      "32057/32057 [==============================] - 67s 2ms/step - loss: 0.8134 - acc: 0.7431 - val_loss: 1.1237 - val_acc: 0.6140\n",
      "Epoch 3/8\n",
      "32057/32057 [==============================] - 65s 2ms/step - loss: 0.7555 - acc: 0.7582 - val_loss: 1.1069 - val_acc: 0.6224\n",
      "Epoch 4/8\n",
      "32057/32057 [==============================] - 63s 2ms/step - loss: 0.7297 - acc: 0.7620 - val_loss: 1.0421 - val_acc: 0.6303\n",
      "Epoch 5/8\n",
      "32057/32057 [==============================] - 62s 2ms/step - loss: 0.7134 - acc: 0.7643 - val_loss: 1.0647 - val_acc: 0.6305\n",
      "Epoch 6/8\n",
      "32057/32057 [==============================] - 62s 2ms/step - loss: 0.6964 - acc: 0.7657 - val_loss: 0.9780 - val_acc: 0.6463\n",
      "Epoch 7/8\n",
      "32057/32057 [==============================] - 62s 2ms/step - loss: 0.6827 - acc: 0.7677 - val_loss: 1.0346 - val_acc: 0.6353\n",
      "Epoch 8/8\n",
      "32057/32057 [==============================] - 63s 2ms/step - loss: 0.6649 - acc: 0.7693 - val_loss: 1.0711 - val_acc: 0.6319\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(word_seq_train, Y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "                 callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       399\n",
      "           1       0.00      0.00      0.00        52\n",
      "           2       0.00      0.00      0.00        14\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       0.75      0.69      0.72        26\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.32      0.34      0.33       500\n",
      "weighted avg       0.71      0.83      0.77       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test.argmax(axis=1), model.predict_classes(word_seq_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
